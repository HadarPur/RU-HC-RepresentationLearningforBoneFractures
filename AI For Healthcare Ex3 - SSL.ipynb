{"cells":[{"cell_type":"markdown","metadata":{"id":"QiOX3P58zXdO"},"source":["# **Exercise 3: Representation learning for bone fractures**\n","\n","## Overview\n","\n","In this assignment you are required to implement a bone fracture xray classification task utilizing a SSL approach with the following data set: https://stanfordmlgroup.github.io/competitions/mura/\n","\"MURA is a dataset of musculoskeletal radiographs consisting of 14,863 studies from 12,173 patients, with a total of 40,561 multi-view radiographic images. Each belongs to one of seven standard upper extremity radiographic study types: elbow, finger, forearm, hand, humerus, shoulder, and wrist. Each study was manually labeled as normal or abnormal by board-certified radiologists from the Stanford Hospital .\n","To evaluate models and get a robust estimate of radiologist performance, we collected additional labels from six board-certified Stanford radiologists on the test set, consisting of 207 musculoskeletal studies.\"\n","\n","<img src=\"https://github.com/HadarPur/RU-HC-RepresentationLearningforBoneFractures/blob/main/figures/radiologist_result_example.png?raw=true\" alt=\"Image\" style=\"max-width: 500px;\" />\n","\n","## Steps\n","1. Please perform data exploration and create a naïve baseline (e.g. can be done based on the paper https://arxiv.org/abs/1712.06957, or any another approach you wish).\n","All steps must include a description of data exploration: data distribution, visualization, thorough evaluation, visualization of results, demonstration of good and bad results.\n","You can focus on the 3 different bones for example – Elbow, Hand and Shoulder as was done in the example https://github.com/Alkoby/Bone-Fracture-Detection:\n","- <img src=\"https://github.com/HadarPur/RU-HC-RepresentationLearningforBoneFractures/blob/main/figures/visualization_example.png?raw=true\" alt=\"Image\" style=\"max-width: 300px;\" />\n","\n","2.  Implement one of the following representation learning approaches listed below and provide a detailed explanation of your approach compared to the baseline (e.g. compare the results when using of 1%,10%,100% of the labeled data as done in https://arxiv.org/pdf/2006.10029.pdf).\n","  * SimCLR Chen et al. https://github.com/google-research/simclr\n","  * Byol Grill et al.https://papers.nips.cc/paper/2020/file/f3ada80d5c4ee70142b17b8192b2958e-Paper.pdf\n","  * Moco He et al. https://arxiv.org/pdf/1911.05722.pdf\n","  * SimSiam Chen et al. https//arxiv.org/abs/2011.10566\n"]},{"cell_type":"markdown","metadata":{"id":"fL_RbmCFON4R"},"source":["# Submitted\n","\n","*   Shir Nitzan\n","*   Timor Baruch\n","*   Hadar Pur"]},{"cell_type":"markdown","metadata":{"id":"B2Ovc12ZMCVz"},"source":["## Imports"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"f7G6jza-UjYM"},"outputs":[],"source":["!pip install torch torchvision pytorch-lightning"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"oZLl8Mq__qXK"},"outputs":[],"source":["import os\n","import torch\n","import multiprocessing\n","import scipy.ndimage\n","\n","import pandas as pd\n","import matplotlib.pyplot as plt\n","import seaborn as sns\n","import numpy as np\n","import torch.nn as nn\n","import torch.nn.functional as F\n","import pytorch_lightning as pl\n","import torch.optim as optim\n","import tensorflow as tf\n","import tensorflow_datasets as tfds\n","\n","from tqdm import tqdm\n","from google.colab import drive\n","from collections import Counter\n","\n","import torch\n","from torch.utils.data import Dataset, DataLoader\n","from torchvision import transforms\n","from PIL import Image\n","from torch.autograd import Variable\n","from psutil import virtual_memory\n","from tabulate import tabulate\n","from PIL import Image\n","from sklearn.model_selection import train_test_split\n","from torchmetrics import Accuracy, Precision, Recall, F1Score\n","from torchvision.models import densenet169\n","from torchvision.transforms.functional import pad\n","from PIL import Image\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"qEtPtH4d-PqQ"},"outputs":[],"source":["import warnings\n","warnings.filterwarnings(\"ignore\", category=UserWarning)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"qYQKOPBO-g7s"},"outputs":[],"source":["mura_v11_path = '/content/MURA-v1.1'\n","if os.path.exists(mura_v11_path) == False:\n","  !gdown 1XjMNPle9fO2NATeXtrIgz6h03LCrwOvN\n","  !unzip -q '/content/MURA-v1.1.zip'\n","  print(\"Done unzip\")\n","else:\n","  print(\"Data exist, continue\")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"LDwl18nyMGpT"},"outputs":[],"source":["print(torch.__version__, torch.cuda.is_available())"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"0CgGTzJ_Q1QF"},"outputs":[],"source":["# change folder name from valid to test\n","!mv /content/MURA-v1.1/valid /content/MURA-v1.1/test"]},{"cell_type":"markdown","metadata":{"id":"NMqqVfWjMQVb"},"source":["## Memory"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"4kMpn-dpMRD6"},"outputs":[],"source":["ram_gb = virtual_memory().total / 1e9\n","print('Your runtime has {:.1f} gigabytes of available RAM\\n'.format(ram_gb))\n","\n","if ram_gb < 20:\n","  print('Not using a high-RAM runtime')\n","else:\n","  print('You are using a high-RAM runtime!')"]},{"cell_type":"markdown","metadata":{"id":"rtWiN1FoMTGA"},"source":["## GPU"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"PxcuQioJMX8X"},"outputs":[],"source":["gpu_info = !nvidia-smi\n","gpu_info = '\\n'.join(gpu_info)\n","if gpu_info.find('failed') >= 0:\n","  print('Not connected to a GPU')\n","else:\n","  print(gpu_info)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"v3RwhuyeMYeD"},"outputs":[],"source":["device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","print(device)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"bi9yY5A9Maem"},"outputs":[],"source":["max_workers = multiprocessing.cpu_count()\n","print(\"Maximum number of workers:\", max_workers)"]},{"cell_type":"markdown","metadata":{"id":"8Od4zPzAOyvt"},"source":["## Prepare data for training"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"iEhPrSy9O2jZ"},"outputs":[],"source":["import os\n","import random\n","from sklearn.model_selection import train_test_split\n","import shutil\n","\n","def split_data(mode):\n","    # Path to the folder containing the train examples\n","    train_folder = \"/content/MURA-v1.1/train\" + \"/\" + mode\n","\n","    # Path to the folder where you want to save the validation images\n","    validation_output_folder = \"/content/MURA-v1.1/valid\" + \"/\" + mode\n","\n","    # Get the list of all image files in the train folder\n","    image_files = [f for f in os.listdir(train_folder) if os.path.isdir(os.path.join(train_folder, f))]\n","\n","    # Split the image files into training and validation sets\n","    if len(image_files) > 0:\n","        train_files, validation_files = train_test_split(image_files, test_size=0.2, random_state=42)\n","\n","        # Move the validation files to the validation output folder\n","        for file in validation_files:\n","            src = os.path.join(train_folder, file)\n","            dst = os.path.join(validation_output_folder, file)\n","            shutil.move(src, dst)\n","    else:\n","        print(\"No image files found in the train folder.\")\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"B53EGhYcTra1"},"outputs":[],"source":["split_data('XR_ELBOW')\n","split_data('XR_HAND')\n","split_data('XR_SHOULDER')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"bZ4fQqDJ1Kp9"},"outputs":[],"source":["def split_data_pct(mode, pct):\n","    # Path to the folder containing the train examples\n","    train_folder = \"/content/MURA-v1.1/train\" + \"/\" + mode\n","\n","    # Get the list of all image files in the train folder\n","    image_files = [f for f in os.listdir(train_folder) if os.path.isdir(os.path.join(train_folder, f))]\n","\n","    # Calculate the number of files based on the percentage\n","    num_files = len(image_files)\n","    num_files_pct = int(num_files * (pct / 100.0))\n","\n","    print(f\"Total num of files is {num_files}\")\n","\n","    # Randomly select the files for the desired percentage\n","    selected_files = random.sample(image_files, num_files_pct)\n","\n","    # Move the selected files to the desired output folder\n","    output_folder = f\"/content/MURA-v1.1/train_{pct}\" + \"/\" + mode\n","    os.makedirs(output_folder, exist_ok=True)\n","    for file in selected_files:\n","        src = os.path.join(train_folder, file)\n","        dst = os.path.join(output_folder, file)\n","        shutil.copytree(src, dst)\n","\n","    # Print the summary\n","    print(f\"Split {pct}%: {num_files_pct} files moved to {output_folder}\")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"WVdetGXM2gtg"},"outputs":[],"source":["# Split the data into different percentages\n","split_data_pct(\"XR_ELBOW\", 1)\n","split_data_pct(\"XR_ELBOW\", 10)\n","split_data_pct(\"XR_ELBOW\", 100)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Pn6iq_au28QS"},"outputs":[],"source":["# Split the data into different percentages\n","split_data_pct(\"XR_HAND\", 1)\n","split_data_pct(\"XR_HAND\", 10)\n","split_data_pct(\"XR_HAND\", 100)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"WKreHTzW2_9h"},"outputs":[],"source":["# Split the data into different percentages\n","split_data_pct(\"XR_SHOULDER\", 1)\n","split_data_pct(\"XR_SHOULDER\", 10)\n","split_data_pct(\"XR_SHOULDER\", 100)"]},{"cell_type":"markdown","metadata":{"id":"K6WhuIr3Ti92"},"source":["## SimCLR"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"SoWGN1mnThOp"},"outputs":[],"source":["!git clone https://github.com/google-research/simclr.git"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Vi0sUXev3HZv"},"outputs":[],"source":["!rm -rf /root/tensorflow_datasets"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"32BY7W4WIgal"},"outputs":[],"source":["!pip install tensorflow_datasets"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"sKMrMTkkIjYj"},"outputs":[],"source":["!tfds --version"]},{"cell_type":"markdown","metadata":{"id":"djnr2w5ZaFgb"},"source":["## Preparing Data\n","The body parts are ['ELBOW', 'HAND', 'SHOULDER']"]},{"cell_type":"markdown","metadata":{"id":"guPmHPLiBNLb"},"source":["### Pre-Proccessing\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"UXwewqWyGxqt"},"outputs":[],"source":["%cd /content/simclr"]},{"cell_type":"markdown","metadata":{"id":"ZJxf0tNwEolz"},"source":["#### Elbow"]},{"cell_type":"markdown","metadata":{"id":"yV9KDw4EBY54"},"source":["###### Elbow 100"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"r_NYsuuvEnjt"},"outputs":[],"source":["!rm -rf /content/simclr/elbow_custom"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"vxxVLf1q7hRZ"},"outputs":[],"source":["%cd /content/simclr"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Bn3ZPOP8DNfo"},"outputs":[],"source":["!tfds new elbow_custom"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ECivTKyaDXcD"},"outputs":[],"source":["%cd /content/simclr/elbow_custom"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"oVlVsq8IxZOn"},"outputs":[],"source":["%%writefile elbow_custom_dataset_builder.py\n","\n","import tensorflow_datasets as tfds\n","import csv\n","import tensorflow as tf\n","from pathlib import Path\n","import numpy as np\n","import os\n","import random\n","\n","class ElbowBuilder(tfds.core.GeneratorBasedBuilder):\n","  \"\"\"DatasetBuilder for elbow_custom dataset.\"\"\"\n","  MANUAL_DOWNLOAD_INSTRUCTIONS = \"/content/simclr\"\n","\n","  VERSION = tfds.core.Version('1.0.0')\n","  RELEASE_NOTES = {'1.0.0': 'Initial release.'}\n","\n","  # Implement the required abstract methods\n","  def _info(self):\n","      # Define the dataset information\n","      features = tfds.features.FeaturesDict({\n","          'image': tfds.features.Image(shape=(None, None, 3)),\n","          'label': tfds.features.ClassLabel(num_classes=2),\n","      })\n","      return tfds.core.DatasetInfo(\n","          builder=self,\n","          description='My custom elbow dataset',\n","          features=features,\n","          supervised_keys=('image', 'label'),\n","      )\n","\n","  def _split_generators(self, dl_manager: tfds.download.DownloadManager):\n","      # Specify the splits\n","      return {\n","          'train': self._generate_examples(Path('/content/MURA-v1.1/train/XR_ELBOW')),\n","          'test': self._generate_examples(Path('/content/MURA-v1.1/test/XR_ELBOW')),\n","          'valid': self._generate_examples(Path('/content/MURA-v1.1/valid/XR_ELBOW'))\n","      }\n","\n","  def _generate_examples(self, path):\n","      for file_name in path.glob('*/*/*.png'):\n","          if 'negative' in str(file_name):\n","              label = 0\n","          elif 'positive' in str(file_name):\n","              label = 1\n","\n","          yield str(file_name), {\n","              'image': file_name,\n","              'label': label,\n","          }"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"4FiXVjr27oEO"},"outputs":[],"source":["!tfds build"]},{"cell_type":"markdown","metadata":{"id":"KpJuwmChz1Ax"},"source":["###### Elbow 10"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"6sSQTtu8z1Ay"},"outputs":[],"source":["!rm -rf /content/simclr/elbow_custom_10"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"hHdhI1siz1Ay"},"outputs":[],"source":["%cd /content/simclr"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"w_Q-RURRz1Ay"},"outputs":[],"source":["!tfds new elbow_custom_10"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Ken0M8nMz1Ay"},"outputs":[],"source":["%cd /content/simclr/elbow_custom_10"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"UedZ-loyz1Ay"},"outputs":[],"source":["%%writefile elbow_custom_10_dataset_builder.py\n","\n","import tensorflow_datasets as tfds\n","import csv\n","import tensorflow as tf\n","from pathlib import Path\n","import numpy as np\n","import os\n","import random\n","\n","class ElbowBuilder10(tfds.core.GeneratorBasedBuilder):\n","  \"\"\"DatasetBuilder for elbow_custom_10 dataset.\"\"\"\n","  MANUAL_DOWNLOAD_INSTRUCTIONS = \"/content/simclr\"\n","\n","  VERSION = tfds.core.Version('1.0.0')\n","  RELEASE_NOTES = {'1.0.0': 'Initial release.'}\n","\n","  # Implement the required abstract methods\n","  def _info(self):\n","      # Define the dataset information\n","      features = tfds.features.FeaturesDict({\n","          'image': tfds.features.Image(shape=(None, None, 3)),\n","          'label': tfds.features.ClassLabel(num_classes=2),\n","      })\n","      return tfds.core.DatasetInfo(\n","          builder=self,\n","          description='My custom elbow dataset',\n","          features=features,\n","          supervised_keys=('image', 'label'),\n","      )\n","\n","  def _split_generators(self, dl_manager: tfds.download.DownloadManager):\n","      # Specify the splits\n","      return {\n","          'train': self._generate_examples(Path('/content/MURA-v1.1/train_10/XR_ELBOW')),\n","          'test': self._generate_examples(Path('/content/MURA-v1.1/test/XR_ELBOW')),\n","          'valid': self._generate_examples(Path('/content/MURA-v1.1/valid/XR_ELBOW'))\n","      }\n","\n","  def _generate_examples(self, path):\n","      for file_name in path.glob('*/*/*.png'):\n","          if 'negative' in str(file_name):\n","              label = 0\n","          elif 'positive' in str(file_name):\n","              label = 1\n","\n","          yield str(file_name), {\n","              'image': file_name,\n","              'label': label,\n","          }"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"AjLW_Eh9z1Ay"},"outputs":[],"source":["!tfds build"]},{"cell_type":"markdown","metadata":{"id":"hZDBN9rO-Clk"},"source":["###### Elbow 1"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"wiJ6h4hN-Clk"},"outputs":[],"source":["!rm -rf /content/simclr/elbow_custom_1"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ot-InrzO-Clk"},"outputs":[],"source":["%cd /content/simclr"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"qphJKpm4-Clk"},"outputs":[],"source":["!tfds new elbow_custom_1"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"2-jIaj3m-Cll"},"outputs":[],"source":["%cd /content/simclr/elbow_custom_1"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"UJzox-PT-Cll"},"outputs":[],"source":["%%writefile elbow_custom_1_dataset_builder.py\n","\n","import tensorflow_datasets as tfds\n","import csv\n","import tensorflow as tf\n","from pathlib import Path\n","import numpy as np\n","import os\n","import random\n","\n","class ElbowBuilder1(tfds.core.GeneratorBasedBuilder):\n","  \"\"\"DatasetBuilder for elbow_custom_1 dataset.\"\"\"\n","  MANUAL_DOWNLOAD_INSTRUCTIONS = \"/content/simclr\"\n","\n","  VERSION = tfds.core.Version('1.0.0')\n","  RELEASE_NOTES = {'1.0.0': 'Initial release.'}\n","\n","  # Implement the required abstract methods\n","  def _info(self):\n","      # Define the dataset information\n","      features = tfds.features.FeaturesDict({\n","          'image': tfds.features.Image(shape=(None, None, 3)),\n","          'label': tfds.features.ClassLabel(num_classes=2),\n","      })\n","      return tfds.core.DatasetInfo(\n","          builder=self,\n","          description='My custom elbow dataset',\n","          features=features,\n","          supervised_keys=('image', 'label'),\n","      )\n","\n","  def _split_generators(self, dl_manager: tfds.download.DownloadManager):\n","      # Specify the splits\n","      return {\n","          'train': self._generate_examples(Path('/content/MURA-v1.1/train_1/XR_ELBOW')),\n","          'test': self._generate_examples(Path('/content/MURA-v1.1/test/XR_ELBOW')),\n","          'valid': self._generate_examples(Path('/content/MURA-v1.1/valid/XR_ELBOW'))\n","      }\n","\n","  def _generate_examples(self, path):\n","      for file_name in path.glob('*/*/*.png'):\n","          if 'negative' in str(file_name):\n","              label = 0\n","          elif 'positive' in str(file_name):\n","              label = 1\n","\n","          yield str(file_name), {\n","              'image': file_name,\n","              'label': label,\n","          }"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"JEEPUu6W-Cll"},"outputs":[],"source":["!tfds build"]},{"cell_type":"markdown","metadata":{"id":"ATXdNaVO1J5S"},"source":["#### Hand"]},{"cell_type":"markdown","metadata":{"id":"fAkYYRrxEvbR"},"source":["###### Hand 100"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"f7RxO9uW1McI"},"outputs":[],"source":["!rm -rf /content/simclr/hand_custom"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"QWD5x6sw1McQ"},"outputs":[],"source":["%cd /content/simclr"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"P5-yXzuk1McQ"},"outputs":[],"source":["!tfds new hand_custom"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"qhHHRHQN1McQ"},"outputs":[],"source":["%cd /content/simclr/hand_custom"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"zk6wIw8i1McQ"},"outputs":[],"source":["%%writefile hand_custom_dataset_builder.py\n","\n","import tensorflow_datasets as tfds\n","import csv\n","import tensorflow as tf\n","from pathlib import Path\n","import numpy as np\n","import os\n","\n","class HandBuilder(tfds.core.GeneratorBasedBuilder):\n","  \"\"\"DatasetBuilder for hand_custom dataset.\"\"\"\n","  MANUAL_DOWNLOAD_INSTRUCTIONS = \"/content/simclr\"\n","\n","  VERSION = tfds.core.Version('1.0.0')\n","  RELEASE_NOTES = {'1.0.0': 'Initial release.'}\n","\n","  # Implement the required abstract methods\n","  def _info(self):\n","      # Define the dataset information\n","      features = tfds.features.FeaturesDict({\n","          'image': tfds.features.Image(shape=(None, None, 3)),\n","          'label': tfds.features.ClassLabel(num_classes=2),\n","      })\n","      return tfds.core.DatasetInfo(\n","          builder=self,\n","          description='My custom hand dataset',\n","          features=features,\n","          supervised_keys=('image', 'label'),\n","      )\n","\n","  def _split_generators(self, dl_manager: tfds.download.DownloadManager):\n","      # Specify the splits\n","      return {\n","          'train': self._generate_examples(Path('/content/MURA-v1.1/train/XR_HAND')),\n","          'test': self._generate_examples(Path('/content/MURA-v1.1/test/XR_HAND')),\n","          'valid': self._generate_examples(Path('/content/MURA-v1.1/valid/XR_HAND'))\n","      }\n","\n","  def _generate_examples(self, path):\n","      for file_name in path.glob('*/*/*.png'):\n","          if 'negative' in str(file_name):\n","              label = 0\n","          elif 'positive' in str(file_name):\n","              label = 1\n","\n","          yield str(file_name), {\n","              'image': file_name,\n","              'label': label,\n","          }"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"jaeaWkoh1McQ"},"outputs":[],"source":["!tfds build"]},{"cell_type":"markdown","metadata":{"id":"oWvMEsz_Ao1E"},"source":["###### Hand 10"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"8Pwq_JVLAo1E"},"outputs":[],"source":["!rm -rf /content/simclr/hand_custom_10"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"T2TYr7dpAo1F"},"outputs":[],"source":["%cd /content/simclr"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"MjTrAozHAo1F"},"outputs":[],"source":["!tfds new hand_custom_10"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"V6Y3xNu2Ao1F"},"outputs":[],"source":["%cd /content/simclr/hand_custom_10"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"F10ZB_veAo1F"},"outputs":[],"source":["%%writefile hand_custom_10_dataset_builder.py\n","\n","import tensorflow_datasets as tfds\n","import csv\n","import tensorflow as tf\n","from pathlib import Path\n","import numpy as np\n","import os\n","\n","class HandBuilder10(tfds.core.GeneratorBasedBuilder):\n","  \"\"\"DatasetBuilder for hand_custom_10 dataset.\"\"\"\n","  MANUAL_DOWNLOAD_INSTRUCTIONS = \"/content/simclr\"\n","\n","  VERSION = tfds.core.Version('1.0.0')\n","  RELEASE_NOTES = {'1.0.0': 'Initial release.'}\n","\n","  # Implement the required abstract methods\n","  def _info(self):\n","      # Define the dataset information\n","      features = tfds.features.FeaturesDict({\n","          'image': tfds.features.Image(shape=(None, None, 3)),\n","          'label': tfds.features.ClassLabel(num_classes=2),\n","      })\n","      return tfds.core.DatasetInfo(\n","          builder=self,\n","          description='My custom hand dataset',\n","          features=features,\n","          supervised_keys=('image', 'label'),\n","      )\n","\n","  def _split_generators(self, dl_manager: tfds.download.DownloadManager):\n","      # Specify the splits\n","      return {\n","          'train': self._generate_examples(Path('/content/MURA-v1.1/train_10/XR_HAND')),\n","          'test': self._generate_examples(Path('/content/MURA-v1.1/test/XR_HAND')),\n","          'valid': self._generate_examples(Path('/content/MURA-v1.1/valid/XR_HAND'))\n","      }\n","\n","  def _generate_examples(self, path):\n","      for file_name in path.glob('*/*/*.png'):\n","          if 'negative' in str(file_name):\n","              label = 0\n","          elif 'positive' in str(file_name):\n","              label = 1\n","\n","          yield str(file_name), {\n","              'image': file_name,\n","              'label': label,\n","          }"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"F11RQP8TAo1F"},"outputs":[],"source":["!tfds build"]},{"cell_type":"markdown","metadata":{"id":"4qP17iQKA9Jc"},"source":["###### Hand 1"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"5xIVT1FaA9Jc"},"outputs":[],"source":["!rm -rf /content/simclr/hand_custom_1"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"q0aIlOYGA9Jd"},"outputs":[],"source":["%cd /content/simclr"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"RWY41cnlA9Jd"},"outputs":[],"source":["!tfds new hand_custom_1"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"dwrCShRNA9Jd"},"outputs":[],"source":["%cd /content/simclr/hand_custom_1"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Foe1YT4bA9Jd"},"outputs":[],"source":["%%writefile hand_custom_1_dataset_builder.py\n","\n","import tensorflow_datasets as tfds\n","import csv\n","import tensorflow as tf\n","from pathlib import Path\n","import numpy as np\n","import os\n","\n","class HandBuilder1(tfds.core.GeneratorBasedBuilder):\n","  \"\"\"DatasetBuilder for hand_custom_1 dataset.\"\"\"\n","  MANUAL_DOWNLOAD_INSTRUCTIONS = \"/content/simclr\"\n","\n","  VERSION = tfds.core.Version('1.0.0')\n","  RELEASE_NOTES = {'1.0.0': 'Initial release.'}\n","\n","  # Implement the required abstract methods\n","  def _info(self):\n","      # Define the dataset information\n","      features = tfds.features.FeaturesDict({\n","          'image': tfds.features.Image(shape=(None, None, 3)),\n","          'label': tfds.features.ClassLabel(num_classes=2),\n","      })\n","      return tfds.core.DatasetInfo(\n","          builder=self,\n","          description='My custom hand dataset',\n","          features=features,\n","          supervised_keys=('image', 'label'),\n","      )\n","\n","  def _split_generators(self, dl_manager: tfds.download.DownloadManager):\n","      # Specify the splits\n","      return {\n","          'train': self._generate_examples(Path('/content/MURA-v1.1/train_1/XR_HAND')),\n","          'test': self._generate_examples(Path('/content/MURA-v1.1/test/XR_HAND')),\n","          'valid': self._generate_examples(Path('/content/MURA-v1.1/valid/XR_HAND'))\n","      }\n","\n","  def _generate_examples(self, path):\n","      for file_name in path.glob('*/*/*.png'):\n","          if 'negative' in str(file_name):\n","              label = 0\n","          elif 'positive' in str(file_name):\n","              label = 1\n","\n","          yield str(file_name), {\n","              'image': file_name,\n","              'label': label,\n","          }"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"1bxuyjZjA9Jd"},"outputs":[],"source":["!tfds build"]},{"cell_type":"markdown","metadata":{"id":"yP3_3E3I1fxT"},"source":["#### Shoulder"]},{"cell_type":"markdown","metadata":{"id":"6tzfyWXrE255"},"source":["###### Shoulder 100"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"nWW44CsG4Xp-"},"outputs":[],"source":["!rm -rf /content/simclr/shoulder_custom"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"W1CPdfu94Xp-"},"outputs":[],"source":["%cd /content/simclr"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"po-xnrUo4Xp_"},"outputs":[],"source":["!tfds new shoulder_custom"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"WfhQpY3Q4Xp_"},"outputs":[],"source":["%cd /content/simclr/shoulder_custom"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"B3zLy8sT4Xp_"},"outputs":[],"source":["%%writefile shoulder_custom_dataset_builder.py\n","\n","import tensorflow_datasets as tfds\n","import csv\n","import tensorflow as tf\n","from pathlib import Path\n","import numpy as np\n","import os\n","\n","class ShoulderBuilder(tfds.core.GeneratorBasedBuilder):\n","  \"\"\"DatasetBuilder for shoulder_custom dataset.\"\"\"\n","  MANUAL_DOWNLOAD_INSTRUCTIONS = \"/content/simclr\"\n","\n","  VERSION = tfds.core.Version('1.0.0')\n","  RELEASE_NOTES = {'1.0.0': 'Initial release.'}\n","\n","  # Implement the required abstract methods\n","  def _info(self):\n","      # Define the dataset information\n","      features = tfds.features.FeaturesDict({\n","          'image': tfds.features.Image(shape=(None, None, 3)),\n","          'label': tfds.features.ClassLabel(num_classes=2),\n","      })\n","      return tfds.core.DatasetInfo(\n","          builder=self,\n","          description='My custom shoulder dataset',\n","          features=features,\n","          supervised_keys=('image', 'label'),\n","      )\n","\n","  def _split_generators(self, dl_manager: tfds.download.DownloadManager):\n","      # Specify the splits\n","      return {\n","          'train': self._generate_examples(Path('/content/MURA-v1.1/train/XR_SHOULDER')),\n","          'test': self._generate_examples(Path('/content/MURA-v1.1/test/XR_SHOULDER')),\n","          'valid': self._generate_examples(Path('/content/MURA-v1.1/valid/XR_SHOULDER'))\n","      }\n","\n","  def _generate_examples(self, path):\n","      for file_name in path.glob('*/*/*.png'):\n","          if 'negative' in str(file_name):\n","              label = 0\n","          elif 'positive' in str(file_name):\n","              label = 1\n","\n","          yield str(file_name), {\n","              'image': file_name,\n","              'label': label,\n","          }"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"yooqvXk04Xp_"},"outputs":[],"source":["!tfds build"]},{"cell_type":"markdown","metadata":{"id":"pWiw5cNFBIwd"},"source":["###### Shoulde 10"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"NdJRe_o7BIwe"},"outputs":[],"source":["!rm -rf /content/simclr/shoulder_custom_10"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"a97fcRYvBIwe"},"outputs":[],"source":["%cd /content/simclr"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"us7qfO0NBIwe"},"outputs":[],"source":["!tfds new shoulder_custom_10"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"UwJaUAmKBIwf"},"outputs":[],"source":["%cd /content/simclr/shoulder_custom_10"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"gNdBybK0BIwf"},"outputs":[],"source":["%%writefile shoulder_custom_10_dataset_builder.py\n","\n","import tensorflow_datasets as tfds\n","import csv\n","import tensorflow as tf\n","from pathlib import Path\n","import numpy as np\n","import os\n","\n","class ShoulderBuilder10(tfds.core.GeneratorBasedBuilder):\n","  \"\"\"DatasetBuilder for shoulder_custom_10 dataset.\"\"\"\n","  MANUAL_DOWNLOAD_INSTRUCTIONS = \"/content/simclr\"\n","\n","  VERSION = tfds.core.Version('1.0.0')\n","  RELEASE_NOTES = {'1.0.0': 'Initial release.'}\n","\n","  # Implement the required abstract methods\n","  def _info(self):\n","      # Define the dataset information\n","      features = tfds.features.FeaturesDict({\n","          'image': tfds.features.Image(shape=(None, None, 3)),\n","          'label': tfds.features.ClassLabel(num_classes=2),\n","      })\n","      return tfds.core.DatasetInfo(\n","          builder=self,\n","          description='My custom shoulder dataset',\n","          features=features,\n","          supervised_keys=('image', 'label'),\n","      )\n","\n","  def _split_generators(self, dl_manager: tfds.download.DownloadManager):\n","      # Specify the splits\n","      return {\n","          'train': self._generate_examples(Path('/content/MURA-v1.1/train_10/XR_SHOULDER')),\n","          'test': self._generate_examples(Path('/content/MURA-v1.1/test/XR_SHOULDER')),\n","          'valid': self._generate_examples(Path('/content/MURA-v1.1/valid/XR_SHOULDER'))\n","      }\n","\n","  def _generate_examples(self, path):\n","      for file_name in path.glob('*/*/*.png'):\n","          if 'negative' in str(file_name):\n","              label = 0\n","          elif 'positive' in str(file_name):\n","              label = 1\n","\n","          yield str(file_name), {\n","              'image': file_name,\n","              'label': label,\n","          }"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"MGxsntlYBIwf"},"outputs":[],"source":["!tfds build"]},{"cell_type":"markdown","metadata":{"id":"z8obmF6LBZUL"},"source":["###### Shoulder 1"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"KlAcswJWBZUL"},"outputs":[],"source":["!rm -rf /content/simclr/shoulder_custom_1"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"9dKMLDNNBZUL"},"outputs":[],"source":["%cd /content/simclr"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"1vWGGXNaBZUL"},"outputs":[],"source":["!tfds new shoulder_custom_1"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"RLoAbYouBZUL"},"outputs":[],"source":["%cd /content/simclr/shoulder_custom_1"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Nl-c1Wd9BZUL"},"outputs":[],"source":["%%writefile shoulder_custom_1_dataset_builder.py\n","\n","import tensorflow_datasets as tfds\n","import csv\n","import tensorflow as tf\n","from pathlib import Path\n","import numpy as np\n","import os\n","\n","class ShoulderBuilder1(tfds.core.GeneratorBasedBuilder):\n","  \"\"\"DatasetBuilder for shoulder_custom_1 dataset.\"\"\"\n","  MANUAL_DOWNLOAD_INSTRUCTIONS = \"/content/simclr\"\n","\n","  VERSION = tfds.core.Version('1.0.0')\n","  RELEASE_NOTES = {'1.0.0': 'Initial release.'}\n","\n","  # Implement the required abstract methods\n","  def _info(self):\n","      # Define the dataset information\n","      features = tfds.features.FeaturesDict({\n","          'image': tfds.features.Image(shape=(None, None, 3)),\n","          'label': tfds.features.ClassLabel(num_classes=2),\n","      })\n","      return tfds.core.DatasetInfo(\n","          builder=self,\n","          description='My custom shoulder dataset',\n","          features=features,\n","          supervised_keys=('image', 'label'),\n","      )\n","\n","  def _split_generators(self, dl_manager: tfds.download.DownloadManager):\n","      # Specify the splits\n","      return {\n","          'train': self._generate_examples(Path('/content/MURA-v1.1/train_1/XR_SHOULDER')),\n","          'test': self._generate_examples(Path('/content/MURA-v1.1/test/XR_SHOULDER')),\n","          'valid': self._generate_examples(Path('/content/MURA-v1.1/valid/XR_SHOULDER'))\n","      }\n","\n","  def _generate_examples(self, path):\n","      for file_name in path.glob('*/*/*.png'):\n","          if 'negative' in str(file_name):\n","              label = 0\n","          elif 'positive' in str(file_name):\n","              label = 1\n","\n","          yield str(file_name), {\n","              'image': file_name,\n","              'label': label,\n","          }"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"_984h9YWBZUL"},"outputs":[],"source":["!tfds build"]},{"cell_type":"markdown","metadata":{"id":"p6eqpC9o4TOI"},"source":["## SimCLR Adjustment"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"J04EneZmzsRT"},"outputs":[],"source":["%cd /content/simclr"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ccigwCAXzozu"},"outputs":[],"source":["%%writefile run.py\n","\n","# coding=utf-8\n","# Copyright 2020 The SimCLR Authors.\n","#\n","# Licensed under the Apache License, Version 2.0 (the \"License\");\n","# you may not use this file except in compliance with the License.\n","# You may obtain a copy of the License at\n","#\n","#     http://www.apache.org/licenses/LICENSE-2.0\n","#\n","# Unless required by applicable law or agreed to in writing, software\n","# distributed under the License is distributed on an \"AS IS\" BASIS,\n","# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n","# See the License for the specific simclr governing permissions and\n","# limitations under the License.\n","# ==============================================================================\n","\"\"\"The main training pipeline.\"\"\"\n","\n","from __future__ import absolute_import\n","from __future__ import division\n","from __future__ import print_function\n","\n","import json\n","import math\n","import os\n","from absl import app\n","from absl import flags\n","\n","import resnet\n","import data as data_lib\n","import model as model_lib\n","import model_util as model_util\n","\n","import tensorflow.compat.v1 as tf\n","from tensorflow.compat.v1 import estimator as tf_estimator\n","import tensorflow_datasets as tfds\n","import tensorflow_hub as hub\n","\n","from elbow_custom.elbow_custom_dataset_builder import ElbowBuilder\n","from elbow_custom_10.elbow_custom_10_dataset_builder import ElbowBuilder10\n","from elbow_custom_1.elbow_custom_1_dataset_builder import ElbowBuilder1\n","\n","from hand_custom.hand_custom_dataset_builder import HandBuilder\n","from hand_custom_10.hand_custom_10_dataset_builder import HandBuilder10\n","from hand_custom_1.hand_custom_1_dataset_builder import HandBuilder1\n","\n","from shoulder_custom.shoulder_custom_dataset_builder import ShoulderBuilder\n","from shoulder_custom_10.shoulder_custom_10_dataset_builder import ShoulderBuilder10\n","from shoulder_custom_1.shoulder_custom_1_dataset_builder import ShoulderBuilder1\n","\n","FLAGS = flags.FLAGS\n","\n","name2builder= {\"ELBOW\" : ElbowBuilder,\n","               \"ELBOW/1\" : ElbowBuilder1,\n","               \"ELBOW/10\" : ElbowBuilder10,\n","               \"ELBOW/100\" : ElbowBuilder,\n","               \"HAND\" : HandBuilder,\n","               \"HAND/1\" : HandBuilder1,\n","               \"HAND/10\" : HandBuilder10,\n","               \"HAND/100\" : HandBuilder,\n","               \"SHOULDER\" : ShoulderBuilder,\n","               \"SHOULDER/1\" : ShoulderBuilder1,\n","               \"SHOULDER/10\" : ShoulderBuilder10,\n","               \"SHOULDER/100\" : ShoulderBuilder}\n","\n","flags.DEFINE_float(\n","    'learning_rate', 0.3,\n","    'Initial learning rate per batch size of 256.')\n","\n","flags.DEFINE_enum(\n","    'learning_rate_scaling', 'linear', ['linear', 'sqrt'],\n","    'How to scale the learning rate as a function of batch size.')\n","\n","flags.DEFINE_float(\n","    'warmup_epochs', 10,\n","    'Number of epochs of warmup.')\n","\n","flags.DEFINE_float(\n","    'weight_decay', 1e-4,\n","    'Amount of weight decay to use.')\n","\n","flags.DEFINE_float(\n","    'batch_norm_decay', 0.9,\n","    'Batch norm decay parameter.')\n","\n","flags.DEFINE_integer(\n","    'train_batch_size', 512,\n","    'Batch size for training.')\n","\n","flags.DEFINE_string(\n","    'train_split', 'train',\n","    'Split for training.')\n","\n","flags.DEFINE_integer(\n","    'train_epochs', 100,\n","    'Number of epochs to train for.')\n","\n","flags.DEFINE_integer(\n","    'train_steps', 0,\n","    'Number of steps to train for. If provided, overrides train_epochs.')\n","\n","flags.DEFINE_integer(\n","    'eval_batch_size', 256,\n","    'Batch size for eval.')\n","\n","flags.DEFINE_integer(\n","    'train_summary_steps', 100,\n","    'Steps before saving training summaries. If 0, will not save.')\n","\n","flags.DEFINE_integer(\n","    'checkpoint_epochs', 1,\n","    'Number of epochs between checkpoints/summaries.')\n","\n","flags.DEFINE_integer(\n","    'checkpoint_steps', 0,\n","    'Number of steps between checkpoints/summaries. If provided, overrides '\n","    'checkpoint_epochs.')\n","\n","flags.DEFINE_string(\n","    'eval_split', 'validation',\n","    'Split for evaluation.')\n","\n","flags.DEFINE_string(\n","    'dataset', 'imagenet2012',\n","    'Name of a dataset.')\n","\n","flags.DEFINE_bool(\n","    'cache_dataset', False,\n","    'Whether to cache the entire dataset in memory. If the dataset is '\n","    'ImageNet, this is a very bad idea, but for smaller datasets it can '\n","    'improve performance.')\n","\n","flags.DEFINE_enum(\n","    'mode', 'train', ['train', 'eval', 'train_then_eval'],\n","    'Whether to perform training or evaluation.')\n","\n","flags.DEFINE_enum(\n","    'train_mode', 'pretrain', ['pretrain', 'finetune'],\n","    'The train mode controls different objectives and trainable components.')\n","\n","flags.DEFINE_string(\n","    'checkpoint', None,\n","    'Loading from the given checkpoint for continued training or fine-tuning.')\n","\n","flags.DEFINE_string(\n","    'variable_schema', '?!global_step',\n","    'This defines whether some variable from the checkpoint should be loaded.')\n","\n","flags.DEFINE_bool(\n","    'zero_init_logits_layer', False,\n","    'If True, zero initialize layers after avg_pool for supervised learning.')\n","\n","flags.DEFINE_integer(\n","    'fine_tune_after_block', -1,\n","    'The layers after which block that we will fine-tune. -1 means fine-tuning '\n","    'everything. 0 means fine-tuning after stem block. 4 means fine-tuning '\n","    'just the linera head.')\n","\n","flags.DEFINE_string(\n","    'master', None,\n","    'Address/name of the TensorFlow master to use. By default, use an '\n","    'in-process master.')\n","\n","flags.DEFINE_string(\n","    'model_dir', None,\n","    'Model directory for training.')\n","\n","flags.DEFINE_string(\n","    'data_dir', None,\n","    'Directory where dataset is stored.')\n","\n","flags.DEFINE_bool(\n","    'use_tpu', True,\n","    'Whether to run on TPU.')\n","\n","tf.flags.DEFINE_string(\n","    'tpu_name', None,\n","    'The Cloud TPU to use for training. This should be either the name '\n","    'used when creating the Cloud TPU, or a grpc://ip.address.of.tpu:8470 '\n","    'url.')\n","\n","tf.flags.DEFINE_string(\n","    'tpu_zone', None,\n","    '[Optional] GCE zone where the Cloud TPU is located in. If not '\n","    'specified, we will attempt to automatically detect the GCE project from '\n","    'metadata.')\n","\n","tf.flags.DEFINE_string(\n","    'gcp_project', None,\n","    '[Optional] Project name for the Cloud TPU-enabled project. If not '\n","    'specified, we will attempt to automatically detect the GCE project from '\n","    'metadata.')\n","\n","flags.DEFINE_enum(\n","    'optimizer', 'lars', ['momentum', 'adam', 'lars'],\n","    'Optimizer to use.')\n","\n","flags.DEFINE_float(\n","    'momentum', 0.9,\n","    'Momentum parameter.')\n","\n","flags.DEFINE_string(\n","    'eval_name', None,\n","    'Name for eval.')\n","\n","flags.DEFINE_integer(\n","    'keep_checkpoint_max', 5,\n","    'Maximum number of checkpoints to keep.')\n","\n","flags.DEFINE_integer(\n","    'keep_hub_module_max', 1,\n","    'Maximum number of Hub modules to keep.')\n","\n","flags.DEFINE_float(\n","    'temperature', 0.1,\n","    'Temperature parameter for contrastive loss.')\n","\n","flags.DEFINE_boolean(\n","    'hidden_norm', True,\n","    'Temperature parameter for contrastive loss.')\n","\n","flags.DEFINE_enum(\n","    'proj_head_mode', 'nonlinear', ['none', 'linear', 'nonlinear'],\n","    'How the head projection is done.')\n","\n","flags.DEFINE_integer(\n","    'proj_out_dim', 128,\n","    'Number of head projection dimension.')\n","\n","flags.DEFINE_integer(\n","    'num_proj_layers', 3,\n","    'Number of non-linear head layers.')\n","\n","flags.DEFINE_integer(\n","    'ft_proj_selector', 0,\n","    'Which layer of the projection head to use during fine-tuning. '\n","    '0 means throwing away the projection head, and -1 means the final layer.')\n","\n","flags.DEFINE_boolean(\n","    'global_bn', True,\n","    'Whether to aggregate BN statistics across distributed cores.')\n","\n","flags.DEFINE_integer(\n","    'width_multiplier', 1,\n","    'Multiplier to change width of network.')\n","\n","flags.DEFINE_integer(\n","    'resnet_depth', 50,\n","    'Depth of ResNet.')\n","\n","flags.DEFINE_float(\n","    'sk_ratio', 0.,\n","    'If it is bigger than 0, it will enable SK. Recommendation: 0.0625.')\n","\n","flags.DEFINE_float(\n","    'se_ratio', 0.,\n","    'If it is bigger than 0, it will enable SE.')\n","\n","flags.DEFINE_integer(\n","    'image_size', 224,\n","    'Input image size.')\n","\n","flags.DEFINE_float(\n","    'color_jitter_strength', 1.0,\n","    'The strength of color jittering.')\n","\n","flags.DEFINE_boolean(\n","    'use_blur', True,\n","    'Whether or not to use Gaussian blur for augmentation during pretraining.')\n","\n","\n","def build_hub_module(model, num_classes, global_step, checkpoint_path):\n","  \"\"\"Create TF-Hub module.\"\"\"\n","\n","  tags_and_args = [\n","      # The default graph is built with batch_norm, dropout etc. in inference\n","      # mode. This graph version is good for inference, not training.\n","      ([], {'is_training': False}),\n","      # A separate \"train\" graph builds batch_norm, dropout etc. in training\n","      # mode.\n","      (['train'], {'is_training': True}),\n","  ]\n","\n","  def module_fn(is_training):\n","    \"\"\"Function that builds TF-Hub module.\"\"\"\n","    endpoints = {}\n","    inputs = tf.placeholder(\n","        tf.float32, [None, None, None, 3])\n","    with tf.variable_scope('base_model', reuse=tf.AUTO_REUSE):\n","      hiddens = model(inputs, is_training)\n","      for v in ['initial_conv', 'initial_max_pool', 'block_group1',\n","                'block_group2', 'block_group3', 'block_group4',\n","                'final_avg_pool']:\n","        endpoints[v] = tf.get_default_graph().get_tensor_by_name(\n","            'base_model/{}:0'.format(v))\n","    if FLAGS.train_mode == 'pretrain':\n","      hiddens_proj = model_util.projection_head(hiddens, is_training)\n","      endpoints['proj_head_input'] = hiddens\n","      endpoints['proj_head_output'] = hiddens_proj\n","    else:\n","      logits_sup = model_util.supervised_head(\n","          hiddens, num_classes, is_training)\n","      endpoints['logits_sup'] = logits_sup\n","    hub.add_signature(inputs=dict(images=inputs),\n","                      outputs=dict(endpoints, default=hiddens))\n","\n","  # Drop the non-supported non-standard graph collection.\n","  drop_collections = ['trainable_variables_inblock_%d'%d for d in range(6)]\n","  spec = hub.create_module_spec(module_fn, tags_and_args, drop_collections)\n","  hub_export_dir = os.path.join(FLAGS.model_dir, 'hub')\n","  checkpoint_export_dir = os.path.join(hub_export_dir, str(global_step))\n","  if tf.io.gfile.exists(checkpoint_export_dir):\n","    # Do not save if checkpoint already saved.\n","    tf.io.gfile.rmtree(checkpoint_export_dir)\n","  spec.export(\n","      checkpoint_export_dir,\n","      checkpoint_path=checkpoint_path,\n","      name_transform_fn=None)\n","\n","  if FLAGS.keep_hub_module_max > 0:\n","    # Delete old exported Hub modules.\n","    exported_steps = []\n","    for subdir in tf.io.gfile.listdir(hub_export_dir):\n","      if not subdir.isdigit():\n","        continue\n","      exported_steps.append(int(subdir))\n","    exported_steps.sort()\n","    for step_to_delete in exported_steps[:-FLAGS.keep_hub_module_max]:\n","      tf.io.gfile.rmtree(os.path.join(hub_export_dir, str(step_to_delete)))\n","\n","\n","def perform_evaluation(estimator, input_fn, eval_steps, model, num_classes,\n","                       checkpoint_path=None):\n","  \"\"\"Perform evaluation.\n","\n","  Args:\n","    estimator: TPUEstimator instance.\n","    input_fn: Input function for estimator.\n","    eval_steps: Number of steps for evaluation.\n","    model: Instance of transfer_learning.models.Model.\n","    num_classes: Number of classes to build model for.\n","    checkpoint_path: Path of checkpoint to evaluate.\n","\n","  Returns:\n","    result: A Dict of metrics and their values.\n","  \"\"\"\n","  if not checkpoint_path:\n","    checkpoint_path = estimator.latest_checkpoint()\n","  result = estimator.evaluate(\n","      input_fn, eval_steps, checkpoint_path=checkpoint_path,\n","      name=FLAGS.eval_name)\n","\n","  # Record results as JSON.\n","  result_json_path = os.path.join(FLAGS.model_dir, 'result.json')\n","  with tf.io.gfile.GFile(result_json_path, 'w') as f:\n","    json.dump({k: float(v) for k, v in result.items()}, f)\n","  result_json_path = os.path.join(\n","      FLAGS.model_dir, 'result_%d.json'%result['global_step'])\n","  with tf.io.gfile.GFile(result_json_path, 'w') as f:\n","    json.dump({k: float(v) for k, v in result.items()}, f)\n","  flag_json_path = os.path.join(FLAGS.model_dir, 'flags.json')\n","\n","  def json_serializable(val):\n","    try:\n","      json.dumps(val)\n","      return True\n","    except TypeError:\n","      return False\n","\n","  with tf.io.gfile.GFile(flag_json_path, 'w') as f:\n","    serializable_flags = {}\n","    for key, val in FLAGS.flag_values_dict().items():\n","      # Some flag value types e.g. datetime.timedelta are not json serializable,\n","      # filter those out.\n","      if json_serializable(val):\n","        serializable_flags[key] = val\n","    json.dump(serializable_flags, f)\n","\n","  # Save Hub module.\n","  build_hub_module(model, num_classes,\n","                   global_step=result['global_step'],\n","                   checkpoint_path=checkpoint_path)\n","\n","  return result\n","\n","\n","def main(argv):\n","  if len(argv) > 1:\n","    raise app.UsageError('Too many command-line arguments.')\n","\n","  # Enable training summary.\n","  if FLAGS.train_summary_steps > 0:\n","    tf.config.set_soft_device_placement(True)\n","\n","  # builder = tfds.builder(FLAGS.dataset, data_dir=FLAGS.data_dir)\n","  # builder.download_and_prepare()\n","\n","  part = FLAGS.dataset\n","  builder = name2builder.get(part)\n","\n","  if (builder is None):\n","    return\n","\n","  builder = builder()\n","  num_train_examples = builder.info.splits[FLAGS.train_split].num_examples\n","  num_eval_examples = builder.info.splits[FLAGS.eval_split].num_examples\n","  num_classes = builder.info.features['label'].num_classes\n","\n","  train_steps = model_util.get_train_steps(num_train_examples)\n","  eval_steps = int(math.ceil(num_eval_examples / FLAGS.eval_batch_size))\n","  epoch_steps = int(round(num_train_examples / FLAGS.train_batch_size))\n","\n","  resnet.BATCH_NORM_DECAY = FLAGS.batch_norm_decay\n","  model = resnet.resnet_v1(\n","      resnet_depth=FLAGS.resnet_depth,\n","      width_multiplier=FLAGS.width_multiplier,\n","      cifar_stem=FLAGS.image_size <= 32)\n","\n","  checkpoint_steps = (\n","      FLAGS.checkpoint_steps or (FLAGS.checkpoint_epochs * epoch_steps))\n","\n","  cluster = None\n","  if FLAGS.use_tpu and FLAGS.master is None:\n","    if FLAGS.tpu_name:\n","      cluster = tf.distribute.cluster_resolver.TPUClusterResolver(\n","          FLAGS.tpu_name, zone=FLAGS.tpu_zone, project=FLAGS.gcp_project)\n","    else:\n","      cluster = tf.distribute.cluster_resolver.TPUClusterResolver()\n","      tf.config.experimental_connect_to_cluster(cluster)\n","      tf.tpu.experimental.initialize_tpu_system(cluster)\n","\n","  default_eval_mode = tf_estimator.tpu.InputPipelineConfig.PER_HOST_V1\n","  sliced_eval_mode = tf_estimator.tpu.InputPipelineConfig.SLICED\n","  run_config = tf_estimator.tpu.RunConfig(\n","      tpu_config=tf_estimator.tpu.TPUConfig(\n","          iterations_per_loop=checkpoint_steps,\n","          eval_training_input_configuration=sliced_eval_mode\n","          if FLAGS.use_tpu else default_eval_mode),\n","      model_dir=FLAGS.model_dir,\n","      save_summary_steps=checkpoint_steps,\n","      save_checkpoints_steps=checkpoint_steps,\n","      keep_checkpoint_max=FLAGS.keep_checkpoint_max,\n","      master=FLAGS.master,\n","      cluster=cluster)\n","  estimator = tf_estimator.tpu.TPUEstimator(\n","      model_lib.build_model_fn(model, num_classes, num_train_examples),\n","      config=run_config,\n","      train_batch_size=FLAGS.train_batch_size,\n","      eval_batch_size=FLAGS.eval_batch_size,\n","      use_tpu=FLAGS.use_tpu)\n","\n","  if FLAGS.mode == 'eval':\n","    for ckpt in tf.train.checkpoints_iterator(\n","        run_config.model_dir, min_interval_secs=15):\n","      try:\n","        result = perform_evaluation(\n","            estimator=estimator,\n","            input_fn=data_lib.build_input_fn(builder, False),\n","            eval_steps=eval_steps,\n","            model=model,\n","            num_classes=num_classes,\n","            checkpoint_path=ckpt)\n","      except tf.errors.NotFoundError:\n","        continue\n","      if result['global_step'] >= train_steps:\n","        return\n","  else:\n","    estimator.train(\n","        data_lib.build_input_fn(builder, True), max_steps=train_steps)\n","    if FLAGS.mode == 'train_then_eval':\n","      perform_evaluation(\n","          estimator=estimator,\n","          input_fn=data_lib.build_input_fn(builder, False),\n","          eval_steps=eval_steps,\n","          model=model,\n","          num_classes=num_classes)\n","\n","\n","if __name__ == '__main__':\n","  tf.disable_v2_behavior()  # Disable eager mode when running with TF2.\n","  app.run(main)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"xu1NDbpc-eJ8"},"outputs":[],"source":["%%writefile model.py\n","\n","# coding=utf-8\n","# Copyright 2020 The SimCLR Authors.\n","#\n","# Licensed under the Apache License, Version 2.0 (the \"License\");\n","# you may not use this file except in compliance with the License.\n","# You may obtain a copy of the License at\n","#\n","#     http://www.apache.org/licenses/LICENSE-2.0\n","#\n","# Unless required by applicable law or agreed to in writing, software\n","# distributed under the License is distributed on an \"AS IS\" BASIS,\n","# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n","# See the License for the specific simclr governing permissions and\n","# limitations under the License.\n","# ==============================================================================\n","\"\"\"Model specification for SimCLR.\"\"\"\n","\n","from __future__ import absolute_import\n","from __future__ import division\n","from __future__ import print_function\n","\n","from absl import flags\n","\n","import data_util as data_util\n","import model_util as model_util\n","import objective as obj_lib\n","\n","import tensorflow.compat.v1 as tf\n","from tensorflow.compat.v1 import estimator as tf_estimator\n","import tensorflow.compat.v2 as tf2\n","\n","FLAGS = flags.FLAGS\n","\n","\n","def build_model_fn(model, num_classes, num_train_examples):\n","  \"\"\"Build model function.\"\"\"\n","  def model_fn(features, labels, mode, params=None):\n","    \"\"\"Build model and optimizer.\"\"\"\n","    is_training = mode == tf_estimator.ModeKeys.TRAIN\n","\n","    # Check training mode.\n","    if FLAGS.train_mode == 'pretrain':\n","      num_transforms = 2\n","      if FLAGS.fine_tune_after_block > -1:\n","        raise ValueError('Does not support layer freezing during pretraining,'\n","                         'should set fine_tune_after_block<=-1 for safety.')\n","    elif FLAGS.train_mode == 'finetune':\n","      num_transforms = 1\n","    else:\n","      raise ValueError('Unknown train_mode {}'.format(FLAGS.train_mode))\n","\n","    # Split channels, and optionally apply extra batched augmentation.\n","    features_list = tf.split(\n","        features, num_or_size_splits=num_transforms, axis=-1)\n","    if FLAGS.use_blur and is_training and FLAGS.train_mode == 'pretrain':\n","      features_list = data_util.batch_random_blur(\n","          features_list, FLAGS.image_size, FLAGS.image_size)\n","    features = tf.concat(features_list, 0)  # (num_transforms * bsz, h, w, c)\n","\n","    # Base network forward pass.\n","    with tf.variable_scope('base_model'):\n","      if FLAGS.train_mode == 'finetune' and FLAGS.fine_tune_after_block >= 4:\n","        # Finetune just supervised (linear) head will not update BN stats.\n","        model_train_mode = False\n","      else:\n","        # Pretrain or finetune anything else will update BN stats.\n","        model_train_mode = is_training\n","      hiddens = model(features, is_training=model_train_mode)\n","\n","    # Add head and loss.\n","    if FLAGS.train_mode == 'pretrain':\n","      tpu_context = params['context'] if 'context' in params else None\n","      hiddens_proj = model_util.projection_head(hiddens, is_training)\n","      contrast_loss, logits_con, labels_con = obj_lib.add_contrastive_loss(\n","          hiddens_proj,\n","          hidden_norm=FLAGS.hidden_norm,\n","          temperature=FLAGS.temperature,\n","          tpu_context=tpu_context if is_training else None)\n","      logits_sup = tf.zeros([params['batch_size'], num_classes])\n","    else:\n","      contrast_loss = tf.zeros([])\n","      logits_con = tf.zeros([params['batch_size'], 10])\n","      labels_con = tf.zeros([params['batch_size'], 10])\n","      hiddens = model_util.projection_head(hiddens, is_training)\n","      logits_sup = model_util.supervised_head(\n","          hiddens, num_classes, is_training)\n","      obj_lib.add_supervised_loss(\n","          labels=labels['labels'],\n","          logits=logits_sup,\n","          weights=labels['mask'])\n","\n","    # Add weight decay to loss, for non-LARS optimizers.\n","    model_util.add_weight_decay(adjust_per_optimizer=True)\n","    loss = tf.losses.get_total_loss()\n","\n","    if FLAGS.train_mode == 'pretrain':\n","      variables_to_train = tf.trainable_variables()\n","    else:\n","      collection_prefix = 'trainable_variables_inblock_'\n","      variables_to_train = []\n","      for j in range(FLAGS.fine_tune_after_block + 1, 6):\n","        variables_to_train += tf.get_collection(collection_prefix + str(j))\n","      assert variables_to_train, 'variables_to_train shouldn\\'t be empty!'\n","\n","    tf.logging.info('===============Variables to train (begin)===============')\n","    tf.logging.info(variables_to_train)\n","    tf.logging.info('================Variables to train (end)================')\n","\n","    learning_rate = model_util.learning_rate_schedule(\n","        FLAGS.learning_rate, num_train_examples)\n","\n","    if is_training:\n","      if FLAGS.train_summary_steps > 0:\n","        # Compute stats for the summary.\n","        prob_con = tf.nn.softmax(logits_con)\n","        entropy_con = - tf.reduce_mean(\n","            tf.reduce_sum(prob_con * tf.math.log(prob_con + 1e-8), -1))\n","\n","        summary_writer = tf2.summary.create_file_writer(FLAGS.model_dir)\n","        # TODO(iamtingchen): remove this control_dependencies in the future.\n","        with tf.control_dependencies([summary_writer.init()]):\n","          with summary_writer.as_default():\n","            should_record = tf.math.equal(\n","                tf.math.floormod(tf.train.get_global_step(),\n","                                 FLAGS.train_summary_steps), 0)\n","            with tf2.summary.record_if(should_record):\n","              contrast_acc = tf.equal(\n","                  tf.argmax(labels_con, 1), tf.argmax(logits_con, axis=1))\n","              contrast_acc = tf.reduce_mean(tf.cast(contrast_acc, tf.float32))\n","              label_acc = tf.equal(\n","                  tf.argmax(labels['labels'], 1), tf.argmax(logits_sup, axis=1))\n","              label_acc = tf.reduce_mean(tf.cast(label_acc, tf.float32))\n","\n","              # Write to a file\n","              #with open('/content/simclr/simclr_pretrain/debug_file.txt', 'w') as f:\n","               # f.write(f\"step: {tf.train.get_global_step() // 30}, current loss: {loss}, contrast acc: {contrast_acc}, label acc: {label_acc}\\n\")\n","\n","              tf2.summary.scalar(\n","                  'train_contrast_loss',\n","                  contrast_loss,\n","                  step=tf.train.get_global_step())\n","              tf2.summary.scalar(\n","                  'train_contrast_acc',\n","                  contrast_acc,\n","                  step=tf.train.get_global_step())\n","              tf2.summary.scalar(\n","                  'train_label_accuracy',\n","                  label_acc,\n","                  step=tf.train.get_global_step())\n","              tf2.summary.scalar(\n","                  'contrast_entropy',\n","                  entropy_con,\n","                  step=tf.train.get_global_step())\n","              tf2.summary.scalar(\n","                  'learning_rate', learning_rate,\n","                  step=tf.train.get_global_step())\n","\n","      optimizer = model_util.get_optimizer(learning_rate)\n","      control_deps = tf.get_collection(tf.GraphKeys.UPDATE_OPS)\n","      if FLAGS.train_summary_steps > 0:\n","        control_deps.extend(tf.summary.all_v2_summary_ops())\n","      with tf.control_dependencies(control_deps):\n","        train_op = optimizer.minimize(\n","            loss, global_step=tf.train.get_or_create_global_step(),\n","            var_list=variables_to_train)\n","\n","      if FLAGS.checkpoint:\n","        def scaffold_fn():\n","          \"\"\"Scaffold function to restore non-logits vars from checkpoint.\"\"\"\n","          tf.train.init_from_checkpoint(\n","              FLAGS.checkpoint,\n","              {v.op.name: v.op.name\n","               for v in tf.global_variables(FLAGS.variable_schema)})\n","\n","          if FLAGS.zero_init_logits_layer:\n","            # Init op that initializes output layer parameters to zeros.\n","            output_layer_parameters = [\n","                var for var in tf.trainable_variables() if var.name.startswith(\n","                    'head_supervised')]\n","            tf.logging.info('Initializing output layer parameters %s to zero',\n","                            [x.op.name for x in output_layer_parameters])\n","            with tf.control_dependencies([tf.global_variables_initializer()]):\n","              init_op = tf.group([\n","                  tf.assign(x, tf.zeros_like(x))\n","                  for x in output_layer_parameters])\n","            return tf.train.Scaffold(init_op=init_op)\n","          else:\n","            return tf.train.Scaffold()\n","      else:\n","        scaffold_fn = None\n","\n","      return tf_estimator.tpu.TPUEstimatorSpec(\n","          mode=mode, train_op=train_op, loss=loss, scaffold_fn=scaffold_fn)\n","    else:\n","\n","      def metric_fn(logits_sup, labels_sup, logits_con, labels_con, mask,\n","                    **kws):\n","        \"\"\"Inner metric function.\"\"\"\n","        metrics = {k: tf.metrics.mean(v, weights=mask)\n","                   for k, v in kws.items()}\n","        metrics['label_accuracy'] = tf.metrics.accuracy(\n","            tf.argmax(labels_sup, 1), tf.argmax(logits_sup, axis=1),\n","            weights=mask)\n","\n","        metrics['contrastive_accuracy'] = tf.metrics.accuracy(\n","            tf.argmax(labels_con, 1), tf.argmax(logits_con, axis=1),\n","            weights=mask)\n","\n","        metrics['recall'] = tf.metrics.recall(\n","            tf.argmax(labels_sup, 1), tf.argmax(logits_sup, axis=1),\n","            weights=mask)\n","\n","        metrics['precision'] = tf.metrics.precision(\n","            tf.argmax(labels_sup, 1), tf.argmax(logits_sup, axis=1),\n","            weights=mask)\n","        return metrics\n","\n","      metrics = {\n","          'logits_sup': logits_sup,\n","          'labels_sup': labels['labels'],\n","          'logits_con': logits_con,\n","          'labels_con': labels_con,\n","          'mask': labels['mask'],\n","          'contrast_loss': tf.fill((params['batch_size'],), contrast_loss),\n","          'regularization_loss': tf.fill((params['batch_size'],),\n","                                         tf.losses.get_regularization_loss()),\n","      }\n","\n","      return tf_estimator.tpu.TPUEstimatorSpec(\n","          mode=mode,\n","          loss=loss,\n","          eval_metrics=(metric_fn, metrics),\n","          scaffold_fn=None)\n","\n","  return model_fn\n"]},{"cell_type":"markdown","metadata":{"id":"V7Fg6J3gQ_gu"},"source":["## SimCLR Experiments"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"S_rbDHXPRuWJ"},"outputs":[],"source":["import json\n","import matplotlib.pyplot as plt\n","import numpy as np\n","\n","def plot_statistics (label):\n","  file_paths = [\n","      f\"/content/simclr/simclr_finetune/{label}/log_1_lr_0.1/result.json\",\n","      f\"/content/simclr/simclr_finetune/{label}/log_10_lr_0.1/result.json\",\n","      f\"/content/simclr/simclr_finetune/{label}/log_100_lr_0.1/result.json\"\n","  ]\n","\n","  # Precentage of images for training\n","  pct_size = [1, 10, 100]\n","\n","  # Create separate lists for precision, recall, and label accuracy\n","  precisions = []\n","  recalls = []\n","  label_accuracies = []\n","\n","  # Read each JSON file and extract the desired values\n","  for file_path in file_paths:\n","      with open(file_path, \"r\") as file:\n","          data = json.load(file)\n","          precision = round(data[\"precision\"], 4)\n","          recall = round(data[\"recall\"], 4)\n","          label_accuracy = round(data[\"label_accuracy\"], 4)\n","          precisions.append(precision)\n","          recalls.append(recall)\n","          label_accuracies.append(label_accuracy)\n","\n","  ############### Label Accuracy #####################\n","\n","  # Plot label accuracy\n","  fig = plt.figure(figsize=(5, 3))\n","  plt.plot(pct_size, label_accuracies, '--', color=\"#000\", marker=\"*\", markeredgecolor=\"#000\", markerfacecolor=\"y\", markersize=16)\n","  plt.xscale(\"log\")\n","  plt.xticks(pct_size, labels=pct_size)\n","  plt.title(f\"MURA-v1.1 {label} Label Classification over dataset size\", fontsize=11)\n","  plt.xlabel(\"Percentage of data\")\n","  plt.ylabel(\"Label Accuracy\")\n","  plt.minorticks_off()\n","  plt.show()\n","\n","  for k, score in zip(pct_size, label_accuracies):\n","      print(f'Label accuracy for {k}% of data: {100 * score:4.2f}%')\n","  print('\\n')\n","\n","\n","  ############### Precision #####################\n","\n","  # Plot Precision\n","  fig = plt.figure(figsize=(5, 3))\n","  plt.plot(pct_size, precisions, '--', color=\"#000\", marker=\"*\", markeredgecolor=\"#000\", markerfacecolor=\"y\", markersize=16)\n","  plt.xscale(\"log\")\n","  plt.xticks(pct_size, labels=pct_size)\n","  plt.title(f\"MURA-v1.1 {label} Precision over dataset size\", fontsize=11)\n","  plt.xlabel(\"Percentage of data\")\n","  plt.ylabel(\"Precision\")\n","  plt.minorticks_off()\n","  plt.show()\n","\n","  for k, score in zip(pct_size, precisions):\n","      print(f'Precision for {k:3}% images per label: {100*score:4.2f}%')\n","  print('\\n')\n","\n","\n","  ############### Recall #####################\n","\n","  # Plot Recall\n","  fig = plt.figure(figsize=(5, 3))\n","  plt.plot(pct_size, recalls, '--', color=\"#000\", marker=\"*\", markeredgecolor=\"#000\", markerfacecolor=\"y\", markersize=16)\n","  plt.xscale(\"log\")\n","  plt.xticks(pct_size, labels=pct_size)\n","  plt.title(f\"MURA-v1.1 {label} Recall over dataset size\", fontsize=11)\n","  plt.xlabel(\"Percentage of data\")\n","  plt.ylabel(\"Recall\")\n","  plt.minorticks_off()\n","  plt.show()\n","\n","  for k, score in zip(pct_size, recalls):\n","      print(f'Recall for {k:3}% images per label: {100*score:4.2f}%')"]},{"cell_type":"markdown","metadata":{"id":"7C2I9x1LRJ1K"},"source":["#### Elbow"]},{"cell_type":"markdown","metadata":{"id":"ZlRYuxSQRJ1P"},"source":["###### Pretrain"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"RGlJHJZsRJ1P"},"outputs":[],"source":["!rm -rf '/content/simclr/simclr_pretrain/elbow'"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"sLTHplU7RJ1P"},"outputs":[],"source":["!python run.py --train_mode=pretrain \\\n","  --train_batch_size=8 --train_epochs=50 --temperature=0.1 \\\n","  --learning_rate=0.075 --learning_rate_scaling=sqrt --weight_decay=1e-4 \\\n","  --dataset=ELBOW --eval_split=valid --resnet_depth=50 \\\n","  --model_dir=/content/simclr/simclr_pretrain/elbow \\\n","  --use_tpu=False --train_summary_steps=100 --eval_batch_size=16"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"3fTDxvhtRJ1P"},"outputs":[],"source":["# Start tensorboard.\n","%reload_ext tensorboard\n","%tensorboard --logdir /content/simclr/simclr_pretrain/elbow"]},{"cell_type":"markdown","metadata":{"id":"nT-XAf67RJ1Q"},"source":["###### Finetune 1 percentage"]},{"cell_type":"markdown","metadata":{"id":"3gMKLBTxRJ1Q"},"source":["###### 1.1 lr=0.3"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"sO3RWdQhRJ1Q"},"outputs":[],"source":["!rm -rf '/content/simclr/simclr_finetune/elbow/log_1_lr_0.3'"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"cPq9jO9mRJ1Q"},"outputs":[],"source":["!python run.py \\\n","  --mode=train_then_eval \\\n","  --train_mode=finetune \\\n","  --fine_tune_after_block=4 \\\n","  --zero_init_logits_layer=True \\\n","  --variable_schema='(?!global_step|(?:.*/|^)Momentum|head)' \\\n","  --global_bn=False \\\n","  --optimizer=momentum \\\n","  --learning_rate=0.3 \\\n","  --weight_decay=0.0 \\\n","  --train_epochs=20 \\\n","  --train_batch_size=8 \\\n","  --warmup_epochs=0 \\\n","  --dataset=ELBOW/1 \\\n","  --eval_split=test \\\n","  --resnet_depth=50 \\\n","  --checkpoint=/content/simclr/simclr_pretrain/elbow \\\n","  --model_dir=/content/simclr/simclr_finetune/elbow/log_1_lr_0.3 \\\n","  --use_tpu=False \\\n","  --eval_batch_size=16"]},{"cell_type":"markdown","metadata":{"id":"0IwdWysFRJ1Q"},"source":["######1.2 lr=0.1"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"EQWEj4RGRJ1Q"},"outputs":[],"source":["!rm -rf '/content/simclr/simclr_finetune/elbow/log_1_lr_0.1'"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"TduVyvXdRJ1Q"},"outputs":[],"source":["!python run.py \\\n","  --mode=train_then_eval \\\n","  --train_mode=finetune \\\n","  --fine_tune_after_block=4 \\\n","  --zero_init_logits_layer=True \\\n","  --variable_schema='(?!global_step|(?:.*/|^)Momentum|head)' \\\n","  --global_bn=False \\\n","  --optimizer=momentum \\\n","  --learning_rate=0.1 \\\n","  --weight_decay=0.0 \\\n","  --train_epochs=20 \\\n","  --train_batch_size=8 \\\n","  --warmup_epochs=0 \\\n","  --dataset=ELBOW/1 \\\n","  --eval_split=test \\\n","  --resnet_depth=50 \\\n","  --checkpoint=/content/simclr/simclr_pretrain/elbow \\\n","  --model_dir=/content/simclr/simclr_finetune/elbow/log_1_lr_0.1 \\\n","  --use_tpu=False \\\n","  --eval_batch_size=16"]},{"cell_type":"markdown","metadata":{"id":"Hn77rAgeRJ1R"},"source":["###### Finetune 10 percentage"]},{"cell_type":"markdown","metadata":{"id":"caslgoKxRJ1R"},"source":["###### 2.1 lr=0.3"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"cnJTsfV7RJ1R"},"outputs":[],"source":["!rm -rf '/content/simclr/simclr_finetune/elbow/log_10_lr_0.3'"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"USiN-9rBRJ1R"},"outputs":[],"source":["!python run.py \\\n","  --mode=train_then_eval \\\n","  --train_mode=finetune \\\n","  --fine_tune_after_block=4 \\\n","  --zero_init_logits_layer=True \\\n","  --variable_schema='(?!global_step|(?:.*/|^)Momentum|head)' \\\n","  --global_bn=False \\\n","  --optimizer=momentum \\\n","  --learning_rate=0.3 \\\n","  --weight_decay=0.0 \\\n","  --train_epochs=20 \\\n","  --train_batch_size=8 \\\n","  --warmup_epochs=0 \\\n","  --dataset=ELBOW/10 \\\n","  --eval_split=test \\\n","  --resnet_depth=50 \\\n","  --checkpoint=/content/simclr/simclr_pretrain/elbow \\\n","  --model_dir=/content/simclr/simclr_finetune/elbow/log_10_lr_0.3 \\\n","  --use_tpu=False \\\n","  --eval_batch_size=16"]},{"cell_type":"markdown","metadata":{"id":"n1_d7ALERJ1R"},"source":["###### 2.2 lr=0.1"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"paMZk87fRJ1R"},"outputs":[],"source":["!rm -rf '/content/simclr/simclr_finetune/elbow/log_10_lr_0.1'"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"6qLHZ2g4RJ1R"},"outputs":[],"source":["!python run.py \\\n","  --mode=train_then_eval \\\n","  --train_mode=finetune \\\n","  --fine_tune_after_block=4 \\\n","  --zero_init_logits_layer=True \\\n","  --variable_schema='(?!global_step|(?:.*/|^)Momentum|head)' \\\n","  --global_bn=False \\\n","  --optimizer=momentum \\\n","  --learning_rate=0.1 \\\n","  --weight_decay=0.0 \\\n","  --train_epochs=20 \\\n","  --train_batch_size=8 \\\n","  --warmup_epochs=0 \\\n","  --dataset=ELBOW/10 \\\n","  --eval_split=test \\\n","  --resnet_depth=50 \\\n","  --checkpoint=/content/simclr/simclr_pretrain/elbow \\\n","  --model_dir=/content/simclr/simclr_finetune/elbow/log_10_lr_0.1 \\\n","  --use_tpu=False \\\n","  --eval_batch_size=16"]},{"cell_type":"markdown","metadata":{"id":"Hv_eRyNpRJ1S"},"source":["###### Finetune 100 percentage"]},{"cell_type":"markdown","metadata":{"id":"KClAgJRPRJ1S"},"source":["######3.1 lr=0.3"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"4Ukykr1uRJ1S"},"outputs":[],"source":["!rm -rf '/content/simclr/simclr_finetune/elbow/log_100_lr_0.3'"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Mr9f86ecRJ1S"},"outputs":[],"source":["!python run.py \\\n","  --mode=train_then_eval \\\n","  --train_mode=finetune \\\n","  --fine_tune_after_block=4 \\\n","  --zero_init_logits_layer=True \\\n","  --variable_schema='(?!global_step|(?:.*/|^)Momentum|head)' \\\n","  --global_bn=False \\\n","  --optimizer=momentum \\\n","  --learning_rate=0.3 \\\n","  --weight_decay=0.0 \\\n","  --train_epochs=30 \\\n","  --train_batch_size=8 \\\n","  --warmup_epochs=0 \\\n","  --dataset=ELBOW/100 \\\n","  --eval_split=test \\\n","  --resnet_depth=50 \\\n","  --checkpoint=/content/simclr/simclr_pretrain/elbow \\\n","  --model_dir=/content/simclr/simclr_finetune/elbow/log_100_lr_0.3 \\\n","  --use_tpu=False \\\n","  --eval_batch_size=16"]},{"cell_type":"markdown","metadata":{"id":"d2mlzFQARJ1S"},"source":["######3.2 lr=0.1"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"lQOuLPH4RJ1S"},"outputs":[],"source":["!rm -rf '/content/simclr/simclr_finetune/elbow/log_100_lr_0.1'"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Qh1oS5qFRJ1S"},"outputs":[],"source":["!python run.py \\\n","  --mode=train_then_eval \\\n","  --train_mode=finetune \\\n","  --fine_tune_after_block=4 \\\n","  --zero_init_logits_layer=True \\\n","  --variable_schema='(?!global_step|(?:.*/|^)Momentum|head)' \\\n","  --global_bn=False \\\n","  --optimizer=momentum \\\n","  --learning_rate=0.1 \\\n","  --weight_decay=0.0 \\\n","  --train_epochs=30 \\\n","  --train_batch_size=8 \\\n","  --warmup_epochs=0 \\\n","  --dataset=ELBOW/100 \\\n","  --eval_split=test \\\n","  --resnet_depth=50 \\\n","  --checkpoint=/content/simclr/simclr_pretrain/elbow \\\n","  --model_dir=/content/simclr/simclr_finetune/elbow/log_100_lr_0.1 \\\n","  --use_tpu=False \\\n","  --eval_batch_size=16"]},{"cell_type":"markdown","metadata":{"id":"KapaibZeR6Y-"},"source":["###### Graphs and Evals"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"yoGH3eBRSCIa"},"outputs":[],"source":["plot_statistics(\"elbow\")"]},{"cell_type":"markdown","metadata":{"id":"vtKoOlKR3aEm"},"source":["#### Hand"]},{"cell_type":"markdown","metadata":{"id":"uauBXHbI3aEs"},"source":["###### Pretrain"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ZAsv-Fqk3aEt"},"outputs":[],"source":["!rm -rf '/content/simclr/simclr_pretrain/hand'"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"K2SHQSlM3aEt"},"outputs":[],"source":["!python run.py --train_mode=pretrain \\\n","  --train_batch_size=8 --train_epochs=50 --temperature=0.1 \\\n","  --learning_rate=0.075 --learning_rate_scaling=sqrt --weight_decay=1e-4 \\\n","  --dataset=HAND --eval_split=valid --resnet_depth=50 \\\n","  --model_dir=/content/simclr/simclr_pretrain/hand \\\n","  --use_tpu=False --train_summary_steps=100 --eval_batch_size=16"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"TAgGnRaI3aEt"},"outputs":[],"source":["# Start tensorboard.\n","%load_ext tensorboard\n","%tensorboard --logdir /content/simclr/simclr_pretrain/hand --port=8010"]},{"cell_type":"markdown","metadata":{"id":"QX5Dw4Sx3aEt"},"source":["###### Finetune 1 percentage"]},{"cell_type":"markdown","metadata":{"id":"aTF9DCPu3aEt"},"source":["######1.1 lr=0.3"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Oc9shBRf3aEt"},"outputs":[],"source":["!rm -rf '/content/simclr/simclr_finetune/hand/log_1_lr_0.3'"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"cQGMrFKP3aEt"},"outputs":[],"source":["!python run.py \\\n","  --mode=train_then_eval \\\n","  --train_mode=finetune \\\n","  --fine_tune_after_block=4 \\\n","  --zero_init_logits_layer=True \\\n","  --variable_schema='(?!global_step|(?:.*/|^)Momentum|head)' \\\n","  --global_bn=False \\\n","  --optimizer=momentum \\\n","  --learning_rate=0.3 \\\n","  --weight_decay=0.0 \\\n","  --train_epochs=20 \\\n","  --train_batch_size=8 \\\n","  --warmup_epochs=0 \\\n","  --dataset=HAND/1 \\\n","  --eval_split=test \\\n","  --resnet_depth=50 \\\n","  --checkpoint=/content/simclr/simclr_pretrain/hand \\\n","  --model_dir=/content/simclr/simclr_finetune/hand/log_1_lr_0.3 \\\n","  --use_tpu=False \\\n","  --eval_batch_size=16"]},{"cell_type":"markdown","metadata":{"id":"HcxQW5JV3aEu"},"source":["######1.2 lr=0.1"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Yhl5Yb9G3aEu"},"outputs":[],"source":["!rm -rf '/content/simclr/simclr_finetune/hand/log_1_lr_0.1'"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"6fjocBfI3aEu"},"outputs":[],"source":["!python run.py \\\n","  --mode=train_then_eval \\\n","  --train_mode=finetune \\\n","  --fine_tune_after_block=4 \\\n","  --zero_init_logits_layer=True \\\n","  --variable_schema='(?!global_step|(?:.*/|^)Momentum|head)' \\\n","  --global_bn=False \\\n","  --optimizer=momentum \\\n","  --learning_rate=0.1 \\\n","  --weight_decay=0.0 \\\n","  --train_epochs=20 \\\n","  --train_batch_size=8 \\\n","  --warmup_epochs=0 \\\n","  --dataset=HAND/1 \\\n","  --eval_split=test \\\n","  --resnet_depth=50 \\\n","  --checkpoint=/content/simclr/simclr_pretrain/hand \\\n","  --model_dir=/content/simclr/simclr_finetune/hand/log_1_lr_0.1 \\\n","  --use_tpu=False \\\n","  --eval_batch_size=16"]},{"cell_type":"markdown","metadata":{"id":"Q3IW_O_a3aEu"},"source":["###### Finetune 10 percentage"]},{"cell_type":"markdown","metadata":{"id":"2mfddtpU3aEu"},"source":["###### 2.1 lr=0.3"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"WlZby_Cd3aEu"},"outputs":[],"source":["!rm -rf '/content/simclr/simclr_finetune/hand/log_10_lr_0.3'"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"NP3sXN0F3aEu"},"outputs":[],"source":["!python run.py \\\n","  --mode=train_then_eval \\\n","  --train_mode=finetune \\\n","  --fine_tune_after_block=4 \\\n","  --zero_init_logits_layer=True \\\n","  --variable_schema='(?!global_step|(?:.*/|^)Momentum|head)' \\\n","  --global_bn=False \\\n","  --optimizer=momentum \\\n","  --learning_rate=0.3 \\\n","  --weight_decay=0.0 \\\n","  --train_epochs=20 \\\n","  --train_batch_size=8 \\\n","  --warmup_epochs=0 \\\n","  --dataset=HAND/10 \\\n","  --eval_split=test \\\n","  --resnet_depth=50 \\\n","  --checkpoint=/content/simclr/simclr_pretrain/hand \\\n","  --model_dir=/content/simclr/simclr_finetune/hand/log_10_lr_0.3 \\\n","  --use_tpu=False \\\n","  --eval_batch_size=16"]},{"cell_type":"markdown","metadata":{"id":"xmVLPgxO3aEu"},"source":["###### 2.2 lr=0.1"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Jz_ynrDJ3aEu"},"outputs":[],"source":["!rm -rf '/content/simclr/simclr_finetune/hand/log_10_lr_0.1'"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"3Utl9eVd3aEv"},"outputs":[],"source":["!python run.py \\\n","  --mode=train_then_eval \\\n","  --train_mode=finetune \\\n","  --fine_tune_after_block=4 \\\n","  --zero_init_logits_layer=True \\\n","  --variable_schema='(?!global_step|(?:.*/|^)Momentum|head)' \\\n","  --global_bn=False \\\n","  --optimizer=momentum \\\n","  --learning_rate=0.1 \\\n","  --weight_decay=0.0 \\\n","  --train_epochs=20 \\\n","  --train_batch_size=8 \\\n","  --warmup_epochs=0 \\\n","  --dataset=HAND/10 \\\n","  --eval_split=test \\\n","  --resnet_depth=50 \\\n","  --checkpoint=/content/simclr/simclr_pretrain/hand \\\n","  --model_dir=/content/simclr/simclr_finetune/hand/log_10_lr_0.1 \\\n","  --use_tpu=False \\\n","  --eval_batch_size=16"]},{"cell_type":"markdown","metadata":{"id":"XHwvavET3aEy"},"source":["###### Finetune 100 percentage"]},{"cell_type":"markdown","metadata":{"id":"OpuI9oV73aEy"},"source":["######3.1 lr=0.3"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"N_vSlS9E3aEy"},"outputs":[],"source":["!rm -rf '/content/simclr/simclr_finetune/hand/log_100_lr_0.3'"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"0naqSx7x3aEy"},"outputs":[],"source":["!python run.py \\\n","  --mode=train_then_eval \\\n","  --train_mode=finetune \\\n","  --fine_tune_after_block=4 \\\n","  --zero_init_logits_layer=True \\\n","  --variable_schema='(?!global_step|(?:.*/|^)Momentum|head)' \\\n","  --global_bn=False \\\n","  --optimizer=momentum \\\n","  --learning_rate=0.3 \\\n","  --weight_decay=0.0 \\\n","  --train_epochs=20 \\\n","  --train_batch_size=8 \\\n","  --warmup_epochs=0 \\\n","  --dataset=HAND/100 \\\n","  --eval_split=test \\\n","  --resnet_depth=50 \\\n","  --checkpoint=/content/simclr/simclr_pretrain/hand \\\n","  --model_dir=/content/simclr/simclr_finetune/hand/log_100_lr_0.3 \\\n","  --use_tpu=False \\\n","  --eval_batch_size=16"]},{"cell_type":"markdown","metadata":{"id":"hfJObdlQ3aEy"},"source":["######3.2 lr=0.1"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"iOv7xeey3aEy"},"outputs":[],"source":["!rm -rf '/content/simclr/simclr_finetune/hand/log_100_lr_0.1'"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"yurSjLnW3aEy"},"outputs":[],"source":["!python run.py \\\n","  --mode=train_then_eval \\\n","  --train_mode=finetune \\\n","  --fine_tune_after_block=4 \\\n","  --zero_init_logits_layer=True \\\n","  --variable_schema='(?!global_step|(?:.*/|^)Momentum|head)' \\\n","  --global_bn=False \\\n","  --optimizer=momentum \\\n","  --learning_rate=0.1 \\\n","  --weight_decay=0.0 \\\n","  --train_epochs=10 \\\n","  --train_batch_size=8 \\\n","  --warmup_epochs=0 \\\n","  --dataset=HAND/100 \\\n","  --eval_split=test \\\n","  --resnet_depth=50 \\\n","  --checkpoint=/content/simclr/simclr_pretrain/hand \\\n","  --model_dir=/content/simclr/simclr_finetune/hand/log_100_lr_0.1 \\\n","  --use_tpu=False \\\n","  --eval_batch_size=16"]},{"cell_type":"markdown","metadata":{"id":"TWZwX_41SYGW"},"source":["###### Graphs and Evals"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"RnFV9BmISeTV"},"outputs":[],"source":["plot_statistics(\"hand\")"]},{"cell_type":"markdown","metadata":{"id":"0PxtwSL84xkx"},"source":["#### Shoulder"]},{"cell_type":"markdown","metadata":{"id":"5jawP2IZS1dt"},"source":["###### Pretrain"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"1r-DZdGMS1dt"},"outputs":[],"source":["!rm -rf '/content/simclr/simclr_pretrain/shoulder'"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Al7KDbw_S1dt"},"outputs":[],"source":["!python run.py --train_mode=pretrain \\\n","  --train_batch_size=8 --train_epochs=50 --temperature=0.1 \\\n","  --learning_rate=0.075 --learning_rate_scaling=sqrt --weight_decay=1e-4 \\\n","  --dataset=SHOULDER --eval_split=valid --resnet_depth=50 \\\n","  --model_dir=/content/simclr/simclr_pretrain/shoulder \\\n","  --use_tpu=False --train_summary_steps=100 --eval_batch_size=16"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"uf4HXCkIS1dt"},"outputs":[],"source":["# Start tensorboard.\n","%load_ext tensorboard\n","%tensorboard --logdir /content/simclr/simclr_pretrain/shoulder --port=8017"]},{"cell_type":"markdown","metadata":{"id":"g0J9mF36S1dt"},"source":["###### Finetune 1 percentage"]},{"cell_type":"markdown","metadata":{"id":"l-4HnHIJS1dt"},"source":["######1.1 lr=0.3"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"cqMFj5tzS1dt"},"outputs":[],"source":["!rm -rf '/content/simclr/simclr_finetune/shoulder/log_1_lr_0.3'"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"iHYWokKDS1dt"},"outputs":[],"source":["!python run.py \\\n","  --mode=train_then_eval \\\n","  --train_mode=finetune \\\n","  --fine_tune_after_block=4 \\\n","  --zero_init_logits_layer=True \\\n","  --variable_schema='(?!global_step|(?:.*/|^)Momentum|head)' \\\n","  --global_bn=False \\\n","  --optimizer=momentum \\\n","  --learning_rate=0.3 \\\n","  --weight_decay=0.0 \\\n","  --train_epochs=20 \\\n","  --train_batch_size=8 \\\n","  --warmup_epochs=0 \\\n","  --dataset=SHOULDER/1 \\\n","  --eval_split=test \\\n","  --resnet_depth=50 \\\n","  --checkpoint=/content/simclr/simclr_pretrain/shoulder \\\n","  --model_dir=/content/simclr/simclr_finetune/shoulder/log_1_lr_0.3 \\\n","  --use_tpu=False \\\n","  --eval_batch_size=16"]},{"cell_type":"markdown","metadata":{"id":"MNssQngWS1du"},"source":["######1.2 lr=0.1"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"FBNyMAF2S1du"},"outputs":[],"source":["!rm -rf '/content/simclr/simclr_finetune/shoulder/log_1_lr_0.1'"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"U1sGn0ASS1du"},"outputs":[],"source":["!python run.py \\\n","  --mode=train_then_eval \\\n","  --train_mode=finetune \\\n","  --fine_tune_after_block=4 \\\n","  --zero_init_logits_layer=True \\\n","  --variable_schema='(?!global_step|(?:.*/|^)Momentum|head)' \\\n","  --global_bn=False \\\n","  --optimizer=momentum \\\n","  --learning_rate=0.1 \\\n","  --weight_decay=0.0 \\\n","  --train_epochs=20 \\\n","  --train_batch_size=8 \\\n","  --warmup_epochs=0 \\\n","  --dataset=SHOULDER/1 \\\n","  --eval_split=test \\\n","  --resnet_depth=50 \\\n","  --checkpoint=/content/simclr/simclr_pretrain/shoulder \\\n","  --model_dir=/content/simclr/simclr_finetune/shoulder/log_1_lr_0.1 \\\n","  --use_tpu=False \\\n","  --eval_batch_size=16"]},{"cell_type":"markdown","metadata":{"id":"G-Ivdzz_S1du"},"source":["###### Finetune 10 percentage"]},{"cell_type":"markdown","metadata":{"id":"PaIIKp1iS1du"},"source":["###### 2.1 lr=0.3"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"3FUqFGf3S1du"},"outputs":[],"source":["!rm -rf '/content/simclr/simclr_finetune/shoulder/log_10_lr_0.3'"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"0LW5usKOS1du"},"outputs":[],"source":["!python run.py \\\n","  --mode=train_then_eval \\\n","  --train_mode=finetune \\\n","  --fine_tune_after_block=4 \\\n","  --zero_init_logits_layer=True \\\n","  --variable_schema='(?!global_step|(?:.*/|^)Momentum|head)' \\\n","  --global_bn=False \\\n","  --optimizer=momentum \\\n","  --learning_rate=0.3 \\\n","  --weight_decay=0.0 \\\n","  --train_epochs=20 \\\n","  --train_batch_size=8 \\\n","  --warmup_epochs=0 \\\n","  --dataset=SHOULDER/10 \\\n","  --eval_split=test \\\n","  --resnet_depth=50 \\\n","  --checkpoint=/content/simclr/simclr_pretrain/shoulder \\\n","  --model_dir=/content/simclr/simclr_finetune/shoulder/log_10_lr_0.3 \\\n","  --use_tpu=False \\\n","  --eval_batch_size=16"]},{"cell_type":"markdown","metadata":{"id":"hmCKovGlS1dv"},"source":["###### 2.2 lr=0.1"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"mq41XD60S1dv"},"outputs":[],"source":["!rm -rf '/content/simclr/simclr_finetune/shoulder/log_10_lr_0.1'"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"cAqGlVV4S1dv"},"outputs":[],"source":["!python run.py \\\n","  --mode=train_then_eval \\\n","  --train_mode=finetune \\\n","  --fine_tune_after_block=4 \\\n","  --zero_init_logits_layer=True \\\n","  --variable_schema='(?!global_step|(?:.*/|^)Momentum|head)' \\\n","  --global_bn=False \\\n","  --optimizer=momentum \\\n","  --learning_rate=0.1 \\\n","  --weight_decay=0.0 \\\n","  --train_epochs=20 \\\n","  --train_batch_size=8 \\\n","  --warmup_epochs=0 \\\n","  --dataset=SHOULDER/10 \\\n","  --eval_split=test \\\n","  --resnet_depth=50 \\\n","  --checkpoint=/content/simclr/simclr_pretrain/shoulder \\\n","  --model_dir=/content/simclr/simclr_finetune/shoulder/log_10_lr_0.1 \\\n","  --use_tpu=False \\\n","  --eval_batch_size=16"]},{"cell_type":"markdown","metadata":{"id":"rF-TNcEES1dv"},"source":["###### Finetune 100 percentage"]},{"cell_type":"markdown","metadata":{"id":"7Bb82-GwS1dv"},"source":["######3.1 lr=0.3"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"STpls6lrS1dv"},"outputs":[],"source":["!rm -rf '/content/simclr/simclr_finetune/shoulder/log_100_lr_0.3'"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"okhdgyFXS1dv"},"outputs":[],"source":["!python run.py \\\n","  --mode=train_then_eval \\\n","  --train_mode=finetune \\\n","  --fine_tune_after_block=4 \\\n","  --zero_init_logits_layer=True \\\n","  --variable_schema='(?!global_step|(?:.*/|^)Momentum|head)' \\\n","  --global_bn=False \\\n","  --optimizer=momentum \\\n","  --learning_rate=0.3 \\\n","  --weight_decay=0.0 \\\n","  --train_epochs=30 \\\n","  --train_batch_size=8 \\\n","  --warmup_epochs=0 \\\n","  --dataset=SHOULDER/100 \\\n","  --eval_split=test \\\n","  --resnet_depth=50 \\\n","  --checkpoint=/content/simclr/simclr_pretrain/shoulder \\\n","  --model_dir=/content/simclr/simclr_finetune/shoulder/log_100_lr_0.3 \\\n","  --use_tpu=False \\\n","  --eval_batch_size=16"]},{"cell_type":"markdown","metadata":{"id":"rGaIKqwuS1dv"},"source":["######3.2 lr=0.1"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"8z50iwoGS1dw"},"outputs":[],"source":["!rm -rf '/content/simclr/simclr_finetune/shoulder/log_100_lr_0.1'"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"t77TXtHF4xk5"},"outputs":[],"source":["!python run.py \\\n","  --mode=train_then_eval \\\n","  --train_mode=finetune \\\n","  --fine_tune_after_block=4 \\\n","  --zero_init_logits_layer=True \\\n","  --variable_schema='(?!global_step|(?:.*/|^)Momentum|head)' \\\n","  --global_bn=False \\\n","  --optimizer=momentum \\\n","  --learning_rate=0.1 \\\n","  --weight_decay=0.0 \\\n","  --train_epochs=30 \\\n","  --train_batch_size=8 \\\n","  --warmup_epochs=0 \\\n","  --dataset=SHOULDER/100 \\\n","  --eval_split=test \\\n","  --resnet_depth=50 \\\n","  --checkpoint=/content/simclr/simclr_pretrain/shoulder \\\n","  --model_dir=/content/simclr/simclr_finetune/shoulder/log_100_lr_0.1 \\\n","  --use_tpu=False \\\n","  --eval_batch_size=16"]},{"cell_type":"markdown","metadata":{"id":"7l2pvwcyTRWG"},"source":["###### Graphs and Evals"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"o8iWpP4IMnTj"},"outputs":[],"source":["plot_statistics(\"shoulder\")"]}],"metadata":{"accelerator":"GPU","colab":{"collapsed_sections":["NMqqVfWjMQVb"],"provenance":[],"toc_visible":true},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}